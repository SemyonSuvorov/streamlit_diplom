from steps import step_upload
import streamlit as st

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
# MODEL_NAME = "cointegrated/rut5-base"
# CACHE_DIR = "models"

def navigation_buttons():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–Ω–æ–ø–æ–∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    col1, col2, col3 = st.columns([1, 3, 1])
    
    with col1:
        inner_col1, inner_col2 = st.columns([1, 2])
        with inner_col1:
            if st.session_state.step > 1:
                if st.button("‚Üê", type="secondary", use_container_width=True):
                    st.session_state.step -= 1
                    st.rerun()
    
    with col3:
        allow_next = False
        if st.session_state.step == 1:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
            allow_next = st.session_state.date_col and st.session_state.target_col

        inner_col1, inner_col2 = st.columns([2, 1])
        with inner_col2:
            if st.session_state.step == 1:
                if allow_next:
                    if st.button("‚Üí", type="primary", use_container_width=True):
                        st.session_state.step += 1
                        st.rerun()
                else:
                    st.button(
                        "‚Üí", 
                        disabled=True, 
                        help="–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π –∏ –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π",
                        use_container_width=True
                    )
            
            elif st.session_state.step == 2:
                if allow_next:
                    if st.button("‚Üí", type="primary", use_container_width=True):
                        st.success("–ü–µ—Ä–µ—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö")
                        # –õ–æ–≥–∏–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞
                else:
                    st.button(
                        "‚Üí", 
                        disabled=True, 
                        help="–í—ã–±–µ—Ä–∏—Ç–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã",
                        use_container_width=True
                    )


def init_session_state():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ session state"""
    session_vars = {
        'step': 1,
        'raw_df': None,
        'processed_df': None,
        'original_columns': [],
        'current_columns': [],
        'temp_columns': [],
        'file_uploaded': False,
        'date_col': None,
        'target_col': None,
        'current_file': None  
    }
    for var, default in session_vars.items():
        if var not in st.session_state:
            st.session_state[var] = default


def main():
    st.set_page_config(page_title="Time-series analysis", page_icon="üìä", layout="wide")
    init_session_state()
    
    st.title("üìä –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤")
    navigation_buttons()
    
    if st.session_state.step == 1:
        st.subheader("–®–∞–≥ 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        step_upload.run_step()
    
    elif st.session_state.step == 2:
        st.session_state.date_col

   


if __name__ == "__main__":
    main()

# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
# @st.cache_resource
# def load_model():
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)
#     model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)
#     return tokenizer, model

# # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
# def generate_new_names(columns, context=""):
#     try:
#         tokenizer, model = load_model()
#         prompt = f"""
#         –ü–µ—Ä–µ–∏–º–µ–Ω—É–π –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –ø–æ—Ä—è–¥–æ–∫.
#         –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}. –ò—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è: {', '.join(columns)}.
#         –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.
#         –ù–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è:"""
        
#         inputs = tokenizer(
#             prompt,
#             return_tensors="pt",
#             max_length=512,
#             truncation=True,
#             padding="max_length"
#         )
        
#         outputs = model.generate(
#             inputs.input_ids,
#             max_length=50,
#             num_beams=5,
#             early_stopping=True
#         )
        
#         new_names = tokenizer.decode(outputs[0], skip_special_tokens=True)
#         new_names = re.sub(r"[^–∞-—è–ê-–Ø0-9,\-_\s]", "", new_names)
#         new_names = [name.strip().replace(' ', '_') for name in new_names.split(",")]
        
#         if len(new_names) != len(columns):
#             return columns
#         return new_names
    
#     except Exception as e:
#         st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
#         return columns