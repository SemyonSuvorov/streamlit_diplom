import streamlit as st
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import re
import hashlib

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
# MODEL_NAME = "cointegrated/rut5-base"
# CACHE_DIR = "models"

def init_session_state():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ session state"""
    session_vars = {
        'raw_df': None,
        'processed_df': None,
        'original_columns': [],
        'current_columns': [],
        'temp_columns': [],
    }
    for var, default in session_vars.items():
        if var not in st.session_state:
            st.session_state[var] = default

def manual_rename_interface():
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤"""
    st.subheader("‚úèÔ∏è –†–µ–¥–∞–∫—Ç–æ—Ä –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if not st.session_state.temp_columns:
        st.session_state.temp_columns = st.session_state.current_columns.copy()

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**–ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ**")
    with col2:
        st.markdown("**–ù–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ**")

    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –Ω–∞–∑–≤–∞–Ω–∏–π
    temp_names = []
    for i, (orig_col, current_col) in enumerate(zip(
        st.session_state.original_columns,
        st.session_state.current_columns
    )):
        row_cols = st.columns(2)
        with row_cols[0]:
            st.code(orig_col)
        with row_cols[1]:
            new_name = st.text_input(
                label=f"–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {orig_col}",
                value=st.session_state.temp_columns[i],
                key=f"col_rename_{i}",
                label_visibility="collapsed"
            )
            temp_names.append(new_name.strip())
    
    st.session_state.temp_columns = temp_names

    # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    btn_col1, btn_col2, btn_col3 = st.columns([1, 2, 1])
    with btn_col1:
        reset_btn = st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω—ã–º", use_container_width=True)

    with btn_col3:
        apply_btn = st.button("‚úÖ –ü—Ä–∏–º–µ–Ω–∏—Ç—å –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è", use_container_width=True)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
    if apply_btn:
        handle_apply_changes()

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–±—Ä–æ—Å–∞
    if reset_btn:
        handle_reset_columns()
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        
    if 'rename_messages' in st.session_state:
        msg = st.session_state.rename_messages
        if msg['type'] == 'error':
            for m in msg['content']:
                st.error(m)
        elif msg['type'] == 'success':
            st.success(msg['content'])
        elif msg['type'] == 'info':
            st.info(msg['content'])
        
        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –ø–æ–∫–∞–∑–∞
        del st.session_state.rename_messages

def handle_apply_changes():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    error_messages = []
    new_columns = st.session_state.temp_columns
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    if any(name == "" for name in new_columns):
        error_messages.append("üö´ –ù–∞–∑–≤–∞–Ω–∏—è –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏!")
    if len(set(new_columns)) != len(new_columns):
        error_messages.append("üö´ –ù–∞–∑–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏!")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ session_state
    if error_messages:
        st.session_state.rename_messages = {'type': 'error', 'content': error_messages}
    else:
        if new_columns == st.session_state.current_columns:
            st.session_state.rename_messages = {'type': 'info', 'content': "‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"}
        else:
            st.session_state.current_columns = new_columns
            st.session_state.processed_df.columns = new_columns
            st.session_state.rename_messages = {'type': 'success', 'content': "‚úÖ –ò–∑–º–µ–Ω–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!"}
        
        st.session_state.temp_columns = new_columns.copy()
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–æ–±—â–µ–Ω–∏–π
    st.rerun()


def handle_reset_columns():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–±—Ä–æ—Å–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
    st.session_state.temp_columns = st.session_state.original_columns.copy()
    st.session_state.current_columns = st.session_state.original_columns.copy()
    st.session_state.processed_df.columns = st.session_state.original_columns
    st.success("üîÑ –ù–∞–∑–≤–∞–Ω–∏—è —Å–±—Ä–æ—à–µ–Ω—ã –∫ –∏—Å—Ö–æ–¥–Ω—ã–º!")

def main():
    st.set_page_config(page_title="Data Assistant", page_icon="üìä", layout="wide")
    init_session_state()
    
    st.title("üìä –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö (CSV/Excel)", type=["csv", "xlsx"])
    
    if uploaded_file:
        try:
            if st.session_state.raw_df is None:
                load_data(uploaded_file)
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
            tab1, tab2, tab3 = st.tabs(["–î–∞–Ω–Ω—ã–µ", "–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ê–Ω–∞–ª–∏–∑"])
            
            with tab1:
                show_data_preview()
            
            with tab2:
                manual_rename_interface()
        
            with tab3:
                #show_analysis_tab()
                date_col = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π",
                    options=st.session_state.current_columns
                )


        except Exception as e:
            st.error(f"üö® –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")
    else:
        st.info("üëÜ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")

def load_data(uploaded_file):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    if uploaded_file.name.endswith('.csv'):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)
    
    st.session_state.update({
        'raw_df': df.copy(),
        'processed_df': df.copy(),
        'original_columns': df.columns.tolist(),
        'current_columns': df.columns.tolist().copy(),
        'temp_columns': df.columns.tolist().copy(),
    })

def show_data_preview():
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    st.subheader("–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é DataFrame —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫
    preview_df = st.session_state.processed_df.copy()
    preview_df.columns = st.session_state.current_columns
    
    st.dataframe(
        preview_df,
        use_container_width=True
    )

def show_analysis_tab():
    """–í–∫–ª–∞–¥–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    st.subheader("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    if st.session_state.processed_df is not None:
        date_col = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–æ–π",
            options=st.session_state.current_columns
        )
        
        try:
            st.session_state.processed_df[date_col] = pd.to_datetime(
                st.session_state.processed_df[date_col]
            )
            st.line_chart(st.session_state.processed_df.set_index(date_col))
        except Exception as e:
            st.error(f"‚è∞ –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞—Ç—ã: {str(e)}")


if __name__ == "__main__":
    main()

# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
# @st.cache_resource
# def load_model():
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)
#     model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)
#     return tokenizer, model

# # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
# def generate_new_names(columns, context=""):
#     try:
#         tokenizer, model = load_model()
#         prompt = f"""
#         –ü–µ—Ä–µ–∏–º–µ–Ω—É–π –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –ø–æ—Ä—è–¥–æ–∫.
#         –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}. –ò—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è: {', '.join(columns)}.
#         –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.
#         –ù–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è:"""
        
#         inputs = tokenizer(
#             prompt,
#             return_tensors="pt",
#             max_length=512,
#             truncation=True,
#             padding="max_length"
#         )
        
#         outputs = model.generate(
#             inputs.input_ids,
#             max_length=50,
#             num_beams=5,
#             early_stopping=True
#         )
        
#         new_names = tokenizer.decode(outputs[0], skip_special_tokens=True)
#         new_names = re.sub(r"[^–∞-—è–ê-–Ø0-9,\-_\s]", "", new_names)
#         new_names = [name.strip().replace(' ', '_') for name in new_names.split(",")]
        
#         if len(new_names) != len(columns):
#             return columns
#         return new_names
    
#     except Exception as e:
#         st.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
#         return columns